{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuliren/anaconda3/envs/rag/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:07<00:00,  2.62s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model_dir = \"./Baichuan2-13B-Chat-v2.0.1\"  # 克隆的仓库路径\n",
    "model_name = \"Baichuan2-13B-Chat\"  # 模型名称\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",  # 自动分配 GPU/CPU\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=\"auto\",  # 自动选择数据类型\n",
    "    low_cpu_mem_usage=True  # 减少 CPU 内存使用\n",
    ").eval()  # 切换到推理模式\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5ef8d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成内容：\n",
      "你是一位资深质量工程师，请详细介绍：\n",
      "1. PFMEA的定义\n",
      "2. 核心三要素\n",
      "3. 典型实施步骤\n",
      "4. 汽车行业的应用案例及挑战分析。\n",
      "耗时：2.28秒\n"
     ]
    }
   ],
   "source": [
    "# 2. 构建符合模型要求的对话格式\n",
    "import time\n",
    "prompt = \"\"\"你是一位资深质量工程师，请详细介绍：\n",
    "1. PFMEA的定义\n",
    "2. 核心三要素\n",
    "3. 典型实施步骤\n",
    "4. 汽车行业的应用案例\"\"\"\n",
    "\n",
    "# 3. 生成优化配置\n",
    "generate_kwargs = {\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"do_sample\": False,    # 关闭随机采样保证确定性结果\n",
    "    \"temperature\": 0.7,     # 可调低至0.3-0.5提升专业性\n",
    "    \"top_p\": 0.9,\n",
    "    \"repetition_penalty\": 1.2,  # 避免重复\n",
    "    \"pad_token_id\": tokenizer.eos_token_id\n",
    "}\n",
    "\n",
    "# 4. 执行推理（首次运行会有编译延迟）\n",
    "start = time.time()\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, **generate_kwargs)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"生成内容：\\n{response}\")\n",
    "print(f\"耗时：{time.time() - start:.2f}秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43a8cae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to run inference\n",
    "def ask_model(prompt, max_new_tokens=512):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62979df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用时间 27.06秒\n",
      "\n",
      "=== 模型输出 ===\n",
      "\n",
      "\n",
      "你是一位资深质量工程师，请根据以下质检数据生成一份完整的 PMEA 报告，内容包括问题描述、原因分析、控制对策建议等。\n",
      "\n",
      "质检数据：\n",
      "产品型号：AXZ-1023\n",
      "缺陷：焊接不牢，导致漏液\n",
      "发现频率：每批10%\n",
      "分析：原材料合金波动 + 焊接设备老化\n",
      "\n",
      "请以专业、清晰的语气输出报告。\n",
      "\n",
      "---\n",
      "\n",
      "AXZ-1023产品质量改进报告\n",
      "\n",
      "一、问题描述\n",
      "\n",
      "近期在产品质量检查过程中发现，AXZ-1023型号产品的焊接部分存在焊接不牢的问题，导致产品在使用过程中出现漏液现象。该问题的发现在每个批次中占比约为10%，属于较为严重的质量问题。\n",
      "\n",
      "二、原因分析\n",
      "\n",
      "通过对质检数据的深入分析，我们认为该问题产生的原因主要包括以下几点：\n",
      "\n",
      "1. 原材料合金波动：由于原材料合金成分的波动，可能导致焊接过程中的熔接效果不佳，从而影响焊接强度。\n",
      "\n",
      "2. 焊接设备老化：长期使用导致焊接设备磨损老化，可能影响焊接过程的稳定性，进而降低焊接质量。\n",
      "\n",
      "三、控制对策建议\n",
      "\n",
      "针对上述原因，我们提出以下质量控制对策建议：\n",
      "\n",
      "1. 对原材料进行更加严格的质量控制，确保合金成分稳定可靠，以降低因原材料问题导致的质量问题。\n",
      "\n",
      "2. 对焊接设备进行全面检查和维护，及时更换老化部件，以确保焊接过程的稳定性和可靠性。\n",
      "\n",
      "3. 加强生产线上的质量管理，对焊接环节进行定期抽查和监控，确保焊接质量得到有效控制。\n",
      "\n",
      "4. 对员工进行焊接技能培训，提高操作技能水平，确保焊接过程严格按照工艺要求进行。\n",
      "\n",
      "通过以上措施的实施，我们期望能够有效地解决AXZ-1023型号产品焊接不牢的问题，从而提高产品质量和客户满意度。\n"
     ]
    }
   ],
   "source": [
    "prompt2 = \"\"\"\n",
    "你是一位资深质量工程师,请介绍一下什么是PFMEA？\n",
    "\"\"\"\n",
    "\n",
    "prompt1 = \"\"\"\n",
    "你是一位资深质量工程师，请根据以下质检数据生成一份完整的 PMEA 报告，内容包括问题描述、原因分析、控制对策建议等。\n",
    "\n",
    "质检数据：\n",
    "产品型号：AXZ-1023\n",
    "缺陷：焊接不牢，导致漏液\n",
    "发现频率：每批10%\n",
    "分析：原材料合金波动 + 焊接设备老化\n",
    "\n",
    "请以专业、清晰的语气输出报告。\n",
    "\"\"\"\n",
    "start = time.time()\n",
    "response = ask_model(prompt1, max_new_tokens=1024)\n",
    "print(f\"使用时间 {time.time() - start:.2f}秒\")\n",
    "print(\"\\n=== 模型输出 ===\\n\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99080b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8109982a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jun 23 21:17:17 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        On  |   00000000:01:00.0 Off |                  Off |\n",
      "|  0%   48C    P8             15W /  450W |   13297MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 4090        On  |   00000000:05:00.0 Off |                  Off |\n",
      "|  0%   50C    P8             33W /  450W |     500MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      2898      G   /usr/lib/xorg/Xorg                            103MiB |\n",
      "|    0   N/A  N/A      3122      G   /usr/bin/gnome-shell                           61MiB |\n",
      "|    0   N/A  N/A     69429      C   ...liren/anaconda3/envs/rag/bin/python      13116MiB |\n",
      "|    1   N/A  N/A      2898      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    1   N/A  N/A     69429      C   ...liren/anaconda3/envs/rag/bin/python        482MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi  # 查看 GPU 使用情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d40ae98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-23 21:35:30 [config.py:823] This model supports multiple tasks: {'generate', 'classify', 'score', 'embed', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 06-23 21:35:30 [config.py:1946] Defaulting to use mp for distributed inference\n",
      "INFO 06-23 21:35:30 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 06-23 21:35:31 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\n",
      "INFO 06-23 21:35:31 [core.py:455] Waiting for init message from front-end.\n",
      "INFO 06-23 21:35:31 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='./Baichuan2-13B-Chat-v2.0.1', speculative_config=None, tokenizer='./Baichuan2-13B-Chat-v2.0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=./Baichuan2-13B-Chat-v2.0.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "WARNING 06-23 21:35:31 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 24 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 06-23 21:35:31 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 16777216, 10, 'psm_4b022686'), local_subscribe_addr='ipc:///tmp/2a4ba1e8-f9f3-4b09-a557-01e80371c699', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 06-23 21:35:31 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fa733126510>\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=73587)\u001b[0;0m INFO 06-23 21:35:31 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_02da7de5'), local_subscribe_addr='ipc:///tmp/fc5bd39b-43d3-48e7-80d3-2a9472b51167', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 06-23 21:35:31 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fa7331265d0>\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=73588)\u001b[0;0m INFO 06-23 21:35:31 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_e944bd93'), local_subscribe_addr='ipc:///tmp/3fb1a175-ac9c-470d-a648-f387b5cb7d24', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=73588)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=73587)\u001b[0;0m INFO 06-23 21:35:32 [utils.py:1126] Found nccl from library libnccl.so.2\n",
      "INFO 06-23 21:35:32 [utils.py:1126] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=73587)\u001b[0;0m INFO 06-23 21:35:32 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=73588)\u001b[0;0m INFO 06-23 21:35:32 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=73587)\u001b[0;0m INFO 06-23 21:35:33 [custom_all_reduce_utils.py:208] generating GPU P2P access cache in /home/xuliren/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=73587)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=73588)\u001b[0;0m INFO 06-23 21:35:55 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /home/xuliren/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "INFO 06-23 21:35:55 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /home/xuliren/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=73587)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=73588)\u001b[0;0m WARNING 06-23 21:35:55 [custom_all_reduce.py:147] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 06-23 21:35:55 [custom_all_reduce.py:147] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=73587)\u001b[0;0m INFO 06-23 21:35:55 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_e96f6bbf'), local_subscribe_addr='ipc:///tmp/365f2f35-c4b8-4bbe-a0e5-4701277a7461', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=73587)\u001b[0;0m INFO 06-23 21:35:55 [parallel_state.py:1065] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=73587)\u001b[0;0m WARNING 06-23 21:35:55 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=73588)\u001b[0;0m INFO 06-23 21:35:55 [parallel_state.py:1065] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=73587)\u001b[0;0m INFO 06-23 21:35:56 [gpu_model_runner.py:1595] Starting to load model ./Baichuan2-13B-Chat-v2.0.1...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=73588)\u001b[0;0m WARNING 06-23 21:35:56 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=73588)\u001b[0;0m INFO 06-23 21:35:56 [gpu_model_runner.py:1595] Starting to load model ./Baichuan2-13B-Chat-v2.0.1...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=73587)\u001b[0;0m INFO 06-23 21:35:56 [gpu_model_runner.py:1600] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=73587)\u001b[0;0m INFO 06-23 21:35:56 [cuda.py:252] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=73588)\u001b[0;0m INFO 06-23 21:35:56 [gpu_model_runner.py:1600] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=73588)\u001b[0;0m INFO 06-23 21:35:56 [cuda.py:252] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cca5579350a24e7e943225428806ea88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=73588)\u001b[0;0m INFO 06-23 21:36:04 [default_loader.py:272] Loading weights took 7.71 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=73587)\u001b[0;0m INFO 06-23 21:36:05 [default_loader.py:272] Loading weights took 8.73 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=73588)\u001b[0;0m INFO 06-23 21:36:05 [gpu_model_runner.py:1624] Model loading took 13.0312 GiB and 7.883787 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=73587)\u001b[0;0m INFO 06-23 21:36:06 [gpu_model_runner.py:1624] Model loading took 13.0312 GiB and 8.886178 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=73588)\u001b[0;0m INFO 06-23 21:36:11 [backends.py:462] Using cache directory: /home/xuliren/.cache/vllm/torch_compile_cache/74d6036712/rank_1_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=73588)\u001b[0;0m INFO 06-23 21:36:11 [backends.py:472] Dynamo bytecode transform time: 5.20 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=73587)\u001b[0;0m INFO 06-23 21:36:12 [backends.py:462] Using cache directory: /home/xuliren/.cache/vllm/torch_compile_cache/74d6036712/rank_0_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=73587)\u001b[0;0m INFO 06-23 21:36:12 [backends.py:472] Dynamo bytecode transform time: 6.06 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=73588)\u001b[0;0m INFO 06-23 21:36:14 [backends.py:161] Cache the graph of shape None for later use\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=73587)\u001b[0;0m INFO 06-23 21:36:14 [backends.py:161] Cache the graph of shape None for later use\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=73588)\u001b[0;0m INFO 06-23 21:36:31 [backends.py:173] Compiling a graph for general shape takes 19.62 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=73587)\u001b[0;0m INFO 06-23 21:36:35 [backends.py:173] Compiling a graph for general shape takes 22.18 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=73587)\u001b[0;0m INFO 06-23 21:36:50 [monitor.py:34] torch.compile takes 28.24 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=73588)\u001b[0;0m INFO 06-23 21:36:50 [monitor.py:34] torch.compile takes 24.81 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=73588)\u001b[0;0m INFO 06-23 21:36:52 [gpu_worker.py:227] Available KV cache memory: 6.90 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=73587)\u001b[0;0m INFO 06-23 21:36:52 [gpu_worker.py:227] Available KV cache memory: 6.89 GiB\n",
      "INFO 06-23 21:36:53 [kv_cache_utils.py:715] GPU KV cache size: 18,064 tokens\n",
      "INFO 06-23 21:36:53 [kv_cache_utils.py:719] Maximum concurrency for 8,192 tokens per request: 2.21x\n",
      "INFO 06-23 21:36:53 [kv_cache_utils.py:715] GPU KV cache size: 18,080 tokens\n",
      "INFO 06-23 21:36:53 [kv_cache_utils.py:719] Maximum concurrency for 8,192 tokens per request: 2.21x\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=73587)\u001b[0;0m INFO 06-23 21:37:46 [gpu_model_runner.py:2048] Graph capturing finished in 53 secs, took 0.80 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=73588)\u001b[0;0m INFO 06-23 21:37:46 [gpu_model_runner.py:2048] Graph capturing finished in 54 secs, took 0.80 GiB\n",
      "INFO 06-23 21:37:46 [core.py:171] init engine (profile, create kv cache, warmup model) took 100.99 seconds\n",
      "WARNING 06-23 21:37:47 [tokenizer.py:262] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "model_path = \"./Baichuan2-13B-Chat-v2.0.1\"  # 模型名称\n",
    "llm = LLM(model=model_path, \n",
    "          trust_remote_code=True,\n",
    "          tensor_parallel_size=2,  # 设置张量并行度\n",
    "          )\n",
    "params = SamplingParams(\n",
    "    max_tokens=512,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebbed7ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98a23ba111e34295969981fe8e80a007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55a951475d4f472aad17cb79fd129cfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== vLLM 输出 ===\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "报告标题：汽车质量工程师转行指导报告\n",
      "\n",
      "\n",
      "\n",
      "报告正文：\n",
      "\n",
      "尊敬的汽车质量工程师转行意向者，\n",
      "\n",
      "首先，感谢您对汽车行业质量工程领域的兴趣。作为一位资深质量工程师，我很高兴为您提供一些专业的建议，以助您顺利实现职业转型。\n",
      "\n",
      "1. **了解汽车行业质量工程**：在开始转型之前，深入了解汽车行业质量工程的定义、职责、工作环境和行业趋势是至关重要的。这将帮助您确立职业目标，并确保您的技能和经验与行业需求相匹配。\n",
      "\n",
      "2. **评估个人技能**：回顾您的教育背景、工作经验和现有技能，评估它们与汽车质量工程师职位要求的匹配程度。识别需要额外学习和发展的领域。\n",
      "\n",
      "3. **获取相关证书**：许多汽车行业雇主倾向于招聘具有专业资格证书的质量工程师。考虑参加ISO/IATF 16949等质量管理体系认证培训，以增强您的职业竞争力。\n",
      "\n",
      "4. **强化专业领域知识**：汽车质量工程师通常需要具备工程知识、质量管理工具和方法、以及车辆测试和验证技能。参加相关课程或在线资源，不断更新和深化这些专业知识。\n",
      "\n",
      "5. **建立行业联系**：参加行业会议、研讨会和工作坊，建立与行业专家和专业人员的联系。这将帮助您了解行业动态，并为未来可能的职业机会建立人脉网络。\n",
      "\n",
      "6. **实习或兼职**：如果您已经具备一定的汽车质量工程经验，考虑在汽车行业寻求实习或兼职机会。这将有助于您获取实际工作经验，并了解行业工作流程。\n",
      "\n",
      "7. **保持学习态度**：汽车行业是快速发展的领域，技术不断更新。保持学习的态度，紧跟行业趋势和新技术，这将有助于您在职业生涯中保持竞争力。\n",
      "\n",
      "8. **准备简历和求职信**：编写一份针对汽车质量工程师职位的简历和求职信。突出您的关键技能、成就和与职位相关的经验。\n",
      "\n",
      "9. **准备面试**：了解汽车行业常见的面试问题和技巧，以便在求职过程中自信地展示您的能力。\n",
      "\n",
      "10. **考虑专业指导**：在转型过程中，寻求一位经验丰富的质量工程师作为导师，提供职业发展的建议和指导。\n",
      "\n",
      "最后，请记住，成功转行需要时间和努力。保持积极的态度，并准备好面对挑战。祝您转型顺利，成为一位杰出的汽车质量工程师。\n",
      "\n",
      "此致，\n",
      "[您的姓名]\n",
      "资深质量工程师\n"
     ]
    }
   ],
   "source": [
    "prompt1 = \"\"\"\n",
    "你是一位资深质量工程师，请告诉我一个想转行做汽车质量工程师的人一些专业的指导建议。\n",
    "\n",
    "\n",
    "\n",
    "请以专业、清晰的语气输出报告。\n",
    "\"\"\"\n",
    "outputs = llm.generate(prompt1, sampling_params=params)\n",
    "print(\"\\n=== vLLM 输出 ===\\n\")\n",
    "print(outputs[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4f5f212",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag.retriever import retrieve  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "022039f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.000257  1.0229094 1.037869  1.0961313 1.1068993]] [[44 43 42 51 46]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve(\"FEMA是什么\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b13bccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'购或服务处理，需求部门后补PR/CER&PO，在OA提交\\nPR/CER时应附上紧急采购申请单附件。\\n4.5.2 正常采购项目\\n公司各部门应根据本部门工作计划及批准的预算合理提前安排本部门的采购事宜。\\n4..5.2.1有年度采购合同\\n年度采购合同的对象和内容：经常消耗的物料或维修服务， 应与供应商签订年度采购合同。\\n4.5.2.1.1 年度采购合同的竞价和签订\\n采购员按如下竞价原则操作并提交相关经理签字审批。\\n1） 对于经常消耗的物料或维修服务类业务采购员按照4.5.3.3竞价原则对比谈判之后确认优选\\n供应商，非最低价中标供应商应由相关领导签字后方可签订年度采购协议。\\n2） 年度采购合同到期前，'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rag.retriever import load_static_faiss\n",
    "index, texts = load_static_faiss()\n",
    "texts[5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
